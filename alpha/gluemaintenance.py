import requests
import pandas as p
import datetime as dt
import os
import threading
from tqdm import tqdm
from time import sleep
import boto3
from os.path import basename
import json
import csv
import gzip
import shutil
import socket
import validatedatabase
import traceback
import logging

class RunGlue:
    def __init__(self, catalog):
        self.client = boto3.client('glue')
        self.catalog = catalog

        self.s3_path = 's3://%s/alpha/data/' % self.catalog
        #print(self.s3_path)
        self.cron = 'cron(15 12 * * ? *)'


        self.database = self.catalog

        self.crawler_name = self.database+'crawler'
        self.role = 'service-role/AWSGlueServiceRole-litcryptodatacrawler'
    def create_crawler(self):

        print('Creating: '+self.crawler_name)

        response = self.client.create_crawler(
            Name=self.crawler_name,
            Role=self.role,
            DatabaseName= self.database,
            Description='autogenerated via gluemaintenance on '+str(dt.datetime.now()),
            Targets={
                'S3Targets': [
                    {
                        'Path': self.s3_path,
                        'Exclusions': []
                    },
                ],
                'JdbcTargets': []
            },
            Schedule=self.cron,
            Classifiers=[],
            TablePrefix='',
            SchemaChangePolicy={
                'UpdateBehavior': 'UPDATE_IN_DATABASE',
                'DeleteBehavior': 'DELETE_FROM_DATABASE'
            },
            Configuration=''
        )

    def run_crawler(self):
        try:
            response = self.client.start_crawler(Name=self.crawler_name)
            print('Triggred: '+self.crawler_name)
        except Exception as e:
            print(e)
            logging.info('------')
            logging.error(traceback.format_exc())
            logging.info('------')
            logging.exception(traceback.format_exc())
            logging.info('------')

    def check_running(self):
        rg = RunGlue(self.catalog)
    #'RUNNING'
        try:
            response = self.client .get_crawlers()
            crawlers = response['Crawlers']
            for crawler in crawlers:

                if crawler['Name'] == self.crawler_name:
                    state = crawler['State']
                    if state == 'READY':
                        rg.run_crawler()
                    else:
                        x = self.crawler_name+' Running Waiting ...'
                        print(x, " ", end="", flush=True)
                        sleep(15)
                        rg.check_running()
                else:
                    pass
        except Exception as e:
            logging.info('------')
            logging.error(traceback.format_exc())
            logging.info('------')
            logging.exception(traceback.format_exc())
            logging.info('------')
            pass
            #print(e)

    def validate_create_crawler(self):
        rg = RunGlue(self.catalog)

        try:
            response = self.client .get_crawlers()
            crawlers = [bucket['Name'] for bucket in response['Crawlers']]
            if self.crawler_name in crawlers:
                print('Validated Crawler: '+self.crawler_name)
                rg.check_running()
            else:
                rg.create_crawler()
                rg.validate_create_crawler()

        except Exception as e:
            logging.info('------')
            logging.error(traceback.format_exc())
            logging.info('------')
            logging.exception(traceback.format_exc())
            logging.info('------')
            pass
            #print(e)


    def main(self):
        try:

            vdd = validatedatabase.CheckDatabase(self.catalog)
            self.catalog = vdd.main()
            rg1 = RunGlue(self.catalog)
            rg1.validate_create_crawler()

        except Exception as e:
            logging.info('------')
            logging.error(traceback.format_exc())
            logging.info('------')
            logging.exception(traceback.format_exc())
            logging.info('------')
            pass
            print(e)

if __name__ == '__main__':

    x = 'litcryptodata'
    rg = RunGlue()
    rg.main()



    #below model for creating crawler:
    """
 
        response = self.client.create_crawler(
            Name=self.crawler_name,
            Role='service-role/AWSGlueServiceRole-lit_crypto',
            DatabaseName='litcrypto',
            Description='autogenerated via managedatastorepy',
            Targets={
                'S3Targets': [
                    {
                        'Path': self.s3_path,
                        'Exclusions': []
                    },
                ],
                'JdbcTargets': [
                    {
                        'ConnectionName': 'string',
                        'Path': 'string',
                        'Exclusions': [
                            'string',
                        ]
                    },
                ]
            },
            Schedule='string',
            Classifiers=[
                'string',
            ],
            TablePrefix='string',
            SchemaChangePolicy={
                'UpdateBehavior': 'LOG'|'UPDATE_IN_DATABASE',
                'DeleteBehavior': 'LOG'|'DELETE_FROM_DATABASE'|'DEPRECATE_IN_DATABASE'
            },
            Configuration='string'
        )

    
    
    """