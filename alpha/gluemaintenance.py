import requests
import pandas as p
import datetime as dt
import os
import threading
from tqdm import tqdm
from time import sleep
import boto3
from os.path import basename
import json
import csv
import gzip
import shutil
import socket
import validatedatabase

class RunGlue:
    def __init__(self, catalog):
        self.client = boto3.client('glue')
        self.catalog = catalog

        self.s3_path = 's3://%s/' % self.catalog
        self.cron = 'cron(15 12 * * ? *)'


        self.database = self.catalog

        self.crawler_name = self.database+'crawler'

    def create_crawler(self):

        print('Creating: '+self.crawler_name)

        response = self.client.create_crawler(
            Name=self.crawler_name,
            Role='service-role/AWSGlueServiceRole-lit_crypto',
            DatabaseName= self.database,
            Description='autogenerated via managedatastorepy',
            Targets={
                'S3Targets': [
                    {
                        'Path': self.s3_path,
                        'Exclusions': []
                    },
                ],
                'JdbcTargets': []
            },
            Schedule=self.cron,
            Classifiers=[],
            TablePrefix='',
            SchemaChangePolicy={
                'UpdateBehavior': 'UPDATE_IN_DATABASE',
                'DeleteBehavior': 'DELETE_FROM_DATABASE'
            },
            Configuration=''
        )

    def run_crawler(self):
        try:
            response = self.client.start_crawler(Name=self.crawler_name)
            print('Triggred: '+self.crawler_name)
        except Exception as e:
            print(e)

    def validate_create_crawler(self):
        rg = RunGlue(self.catalog)

        try:
            response = self.client .get_crawlers()
            crawlers = [bucket['Name'] for bucket in response['Crawlers']]
            if self.crawler_name in crawlers:
                print('Validated Crawler: '+self.crawler_name)
                rg.run_crawler()
            else:
                rg.create_crawler()
                print('Creating Crawler: '+self.crawler_name)
                rg.validate_create_crawler()

        except Exception as e:
            pass
            print(e)


    def main(self):
        try:

            vdd = validatedatabase.CheckDatabase(self.catalog)
            self.catalog = vdd.main()
            rg1 = RunGlue(self.catalog)
            rg1.validate_create_crawler()

        except Exception as e:
            pass
            print(e)

if __name__ == '__main__':

    #x = 'litcryptodata'
    rg = RunGlue()
    rg.main()



    #below model for creating crawler:
    """
 
        response = self.client.create_crawler(
            Name=self.crawler_name,
            Role='service-role/AWSGlueServiceRole-lit_crypto',
            DatabaseName='litcrypto',
            Description='autogenerated via managedatastorepy',
            Targets={
                'S3Targets': [
                    {
                        'Path': self.s3_path,
                        'Exclusions': []
                    },
                ],
                'JdbcTargets': [
                    {
                        'ConnectionName': 'string',
                        'Path': 'string',
                        'Exclusions': [
                            'string',
                        ]
                    },
                ]
            },
            Schedule='string',
            Classifiers=[
                'string',
            ],
            TablePrefix='string',
            SchemaChangePolicy={
                'UpdateBehavior': 'LOG'|'UPDATE_IN_DATABASE',
                'DeleteBehavior': 'LOG'|'DELETE_FROM_DATABASE'|'DEPRECATE_IN_DATABASE'
            },
            Configuration='string'
        )

    
    
    """